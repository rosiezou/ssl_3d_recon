{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CadToPointCloudProto.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICbXZMnSympG"
      },
      "source": [
        "Prototype code to randomly sample pointcloud from a CAD file belonging to Shapenet. \n",
        "https://towardsdatascience.com/deep-learning-on-point-clouds-implementing-pointnet-in-google-colab-1fd65cd3a263 played an elemental role in understanding how to convert CAD files into pointclouds!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo1QyxXOe1Kp"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "!pip install path.py;\n",
        "from path import Path\n",
        "import plotly.graph_objects as go\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G7kwJSxgdtN"
      },
      "source": [
        "!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
        "!unzip -q ModelNet10.zip\n",
        "\n",
        "path = Path(\"ModelNet10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecTfc9rOPQI-"
      },
      "source": [
        "# To prototype idea on a single image.\n",
        "# ! wget http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v1/02958343/4489a777dd90ebcce28605e174815eef/model.obj \n",
        "! wget http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v1/03001627/c41fe0605cfe70571c25d54737ed5c8e/model.obj "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oEBu5LknULv"
      },
      "source": [
        "# Code forked from https://github.com/thethoughtfulgeek/bpy-visualization-utils/blob/master/mesh.py\n",
        "\n",
        "def write_off(file, vertices, faces):\n",
        "    \"\"\"\n",
        "    Writes the given vertices and faces to OFF.\n",
        "    :param vertices: vertices as tuples of (x, y, z) coordinates\n",
        "    :type vertices: [(float)]\n",
        "    :param faces: faces as tuples of (num_vertices, vertex_id_1, vertex_id_2, ...)\n",
        "    :type faces: [(int)]\n",
        "    \"\"\"\n",
        "\n",
        "    num_vertices = len(vertices)\n",
        "    num_faces = len(faces)\n",
        "\n",
        "    assert num_vertices > 0\n",
        "    assert num_faces > 0\n",
        "\n",
        "    with open(file, 'w') as fp:\n",
        "        fp.write('OFF\\n')\n",
        "        fp.write(str(num_vertices) + ' ' + str(num_faces) + ' 0\\n')\n",
        "\n",
        "        for vertex in vertices:\n",
        "            assert len(vertex) == 3, 'invalid vertex with %d dimensions found (%s)' % (len(vertex), file)\n",
        "            fp.write(str(vertex[0]) + ' ' + str(vertex[1]) + ' ' + str(vertex[2]) + '\\n')\n",
        "\n",
        "        for face in faces:\n",
        "            assert face[0] == 3, 'only triangular faces supported (%s)' % file\n",
        "            assert len(face) == 4, 'faces need to have 3 vertices, but found %d (%s)' % (len(face), file)\n",
        "\n",
        "            for i in range(len(face)):\n",
        "                assert face[i] >= 0 and face[i] < num_vertices, 'invalid vertex index %d (of %d vertices) (%s)' % (face[i], num_vertices, file)\n",
        "\n",
        "                fp.write(str(face[i]))\n",
        "                if i < len(face) - 1:\n",
        "                    fp.write(' ')\n",
        "\n",
        "            fp.write('\\n')\n",
        "\n",
        "        # add empty line to be sure\n",
        "        fp.write('\\n')\n",
        "\n",
        "def read_obj(file):\n",
        "    \"\"\"\n",
        "    Reads vertices and faces from an obj file.\n",
        "    :param file: path to file to read\n",
        "    :type file: str\n",
        "    :return: vertices and faces as lists of tuples\n",
        "    :rtype: [(float)], [(int)]\n",
        "    \"\"\"\n",
        "\n",
        "    assert os.path.exists(file), 'file %s not found' % file\n",
        "\n",
        "    with open(file, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "        lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        vertices = []\n",
        "        faces = []\n",
        "        for line in lines:\n",
        "            parts = line.split(' ')\n",
        "            parts = [part.strip() for part in parts if part]\n",
        "            if parts[0] == 'v':\n",
        "                assert len(parts) == 4, \\\n",
        "                    'vertex should be of the form v x y z, but found %d parts instead (%s)' % (len(parts), file)\n",
        "                assert parts[1] != '', 'vertex x coordinate is empty (%s)' % file\n",
        "                assert parts[2] != '', 'vertex y coordinate is empty (%s)' % file\n",
        "                assert parts[3] != '', 'vertex z coordinate is empty (%s)' % file\n",
        "\n",
        "                vertices.append([float(parts[1]), float(parts[2]), float(parts[3])])\n",
        "            elif parts[0] == 'f':\n",
        "                assert len(parts) == 4, \\\n",
        "                    'face should be of the form f v1/vt1/vn1 v2/vt2/vn2 v2/vt2/vn2, but found %d parts (%s) instead (%s)' % (len(parts), line, file)\n",
        "\n",
        "                components = parts[1].split('/')\n",
        "                assert len(components) >= 1 and len(components) <= 3, \\\n",
        "                   'face component should have the forms v, v/vt or v/vt/vn, but found %d components instead (%s)' % (len(components), file)\n",
        "                assert components[0].strip() != '', \\\n",
        "                    'face component is empty (%s)' % file\n",
        "                v1 = int(components[0])\n",
        "\n",
        "                components = parts[2].split('/')\n",
        "                assert len(components) >= 1 and len(components) <= 3, \\\n",
        "                    'face component should have the forms v, v/vt or v/vt/vn, but found %d components instead (%s)' % (len(components), file)\n",
        "                assert components[0].strip() != '', \\\n",
        "                    'face component is empty (%s)' % file\n",
        "                v2 = int(components[0])\n",
        "\n",
        "                components = parts[3].split('/')\n",
        "                assert len(components) >= 1 and len(components) <= 3, \\\n",
        "                    'face component should have the forms v, v/vt or v/vt/vn, but found %d components instead (%s)' % (len(components), file)\n",
        "                assert components[0].strip() != '', \\\n",
        "                    'face component is empty (%s)' % file\n",
        "                v3 = int(components[0])\n",
        "\n",
        "                #assert v1 != v2 and v2 != v3 and v3 != v2, 'degenerate face detected: %d %d %d (%s)' % (v1, v2, v3, file)\n",
        "                if v1 == v2 or v2 == v3 or v1 == v3:\n",
        "                    # print('[Info] skipping degenerate face in %s' % file)\n",
        "                    continue\n",
        "                else:\n",
        "                    faces.append([v1 - 1, v2 - 1, v3 - 1]) # indices are 1-based!\n",
        "            else:\n",
        "              # continue to next line until one of the required args is found.\n",
        "              continue\n",
        "        return np.array(vertices, dtype=float), np.array(faces, dtype=int)\n",
        "\n",
        "    assert False, 'could not open %s' % file"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jBUgWBbsQig"
      },
      "source": [
        "# Convert obj to an off file and save locally\n",
        "def convert_obj_to_off(obj_file, output_filename):\n",
        "  if not os.path.exists(obj_file):\n",
        "    print(obj_file, ' does not exist.')\n",
        "    return\n",
        "  obj_vertices, obj_faces = read_obj(obj_file)\n",
        "  assert obj_vertices.shape[1] == 3\n",
        "  assert obj_faces.shape[1] == 3\n",
        "  temp_faces = np.ones((obj_faces.shape[0], 4), dtype = int)*3\n",
        "  # print(obj_faces.shape)\n",
        "  temp_faces[:, 1:4] = obj_faces[:, :]\n",
        "  write_off(output_filename, obj_vertices.tolist(), temp_faces.tolist())"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNHP3lmhCcu"
      },
      "source": [
        "def read_cad_off(filename):\n",
        "  # Read CAD files with .off extension.\n",
        "  with open(filename, 'r') as f:\n",
        "    if 'OFF' != f.readline().strip():\n",
        "      print(f'Error: {filename} is a not a file.')\n",
        "      return false\n",
        "    num_vertices, num_faces, _ = tuple([int(s) for s in f.readline().strip().split(' ')])\n",
        "    vertices = [[float(s) for s in f.readline().strip().split(' ')] for ith_vertex in range(num_vertices)]\n",
        "    faces = [[int(s) for s in f.readline().strip().split(' ')][1:] for ith_face in range(num_faces)]\n",
        "    return vertices, faces"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLJlk1j9iY4c"
      },
      "source": [
        "# In case of obj files, convert to off files first\n",
        "sample_file = '/content/model.off'\n",
        "convert_obj_to_off('/content/model.obj', sample_file)\n",
        "\n",
        "# Read the sample CAD file and plot the mesh.\n",
        "# Works with direct off files.\n",
        "# sample_file = path/\"bed/train/bed_0001.off\"\n",
        "off_vertices, off_faces = read_cad_off(sample_file)\n",
        "\n",
        "\n",
        "\n",
        "plotting_vertex = np.array(off_vertices).T\n",
        "fig = go.Figure(data=[go.Mesh3d(x=plotting_vertex[0], y=plotting_vertex[1], z=plotting_vertex[2], color='lightpink', opacity=0.50)])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7rlWyf3rKNb"
      },
      "source": [
        "\n",
        "# Same bed plot without the meshes (Points only)\n",
        "fig = go.Figure(data=[go.Scatter3d(x=plotting_vertex[0], y=plotting_vertex[1], z=plotting_vertex[2],\n",
        "                                   mode='markers', marker=dict(size=1))])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgJe7UAGrcqM"
      },
      "source": [
        "# Calculate area for each face\n",
        "areas = np.zeros((len(off_faces)))\n",
        "off_vertices = np.array(off_vertices)\n",
        "\n",
        "def calculate_triangle_area(pt1, pt2, pt3):\n",
        "  # Calculate the area of the triangle formed by the provided points.\n",
        "  # Note: This area calculation is for a 3D triangle with Heron's formula\n",
        "  side_a = np.linalg.norm(pt1 - pt2)\n",
        "  side_b = np.linalg.norm(pt2 - pt3)\n",
        "  side_c = np.linalg.norm(pt3 - pt1)\n",
        "  perimeter = 0.5 * ( side_a + side_b + side_c)\n",
        "  return max(perimeter * (perimeter - side_a) * (perimeter - side_b) * (perimeter - side_c), 0)**0.5\n",
        "\n",
        "\n",
        "for i in range(len(areas)):\n",
        "    areas[i] = calculate_triangle_area(off_vertices[off_faces[i][0]], off_vertices[off_faces[i][1]], off_vertices[off_faces[i][2]])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hfMSUSL7jD1"
      },
      "source": [
        "# Sample to a fixed number of points in each face.\n",
        "# The probability of choosing a face is proportional to its area.\n",
        "# The point distribution should be uniform for each face.\n",
        "k = 1024\n",
        "sampled_faces = random.choices(off_faces, weights=areas, k=k)\n",
        "\n",
        "# Sample points on the surface of the chosen triangle\n",
        "def sample_point_on_triangle(pt1, pt2, pt3):\n",
        "    # barycentric coordinates on a triangle\n",
        "    # https://mathworld.wolfram.com/BarycentricCoordinates.html\n",
        "    # Another good reference: https://pharr.org/matt/blog/2019/02/27/triangle-sampling-1.html\n",
        "    s, t = sorted([random.random(), random.random()])\n",
        "    f = lambda i: s * pt1[i] + (t-s) * pt2[i] + (1-t) * pt3[i]\n",
        "    return (f(0), f(1), f(2))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKK4ftmQ9w-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f73ac1-0e3c-40bd-f607-1cb9fece038c"
      },
      "source": [
        "# Construct pointcloud.\n",
        "pointcloud = np.zeros((k, 3))\n",
        "for i in range(len(sampled_faces)):\n",
        "    pointcloud[i] = (sample_point_on_triangle(off_vertices[sampled_faces[i][0]], off_vertices[sampled_faces[i][1]], off_vertices[sampled_faces[i][2]]))\n",
        "\n",
        "print(len(sampled_faces))\n",
        "\n",
        "# Save pointcloud to a npy file.\n",
        "np_filename = '/content/gt_pointcloud_1024.npy'\n",
        "np.save(np_filename, pointcloud, allow_pickle=True)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8TgdxstDPrG"
      },
      "source": [
        "# Plot pointcloud.\n",
        "# Load pointcloud from pickle file\n",
        "np_filename = '/content/gt_pointcloud_1024.npy'\n",
        "loaded_pointcloud = np.load(np_filename, allow_pickle=True)\n",
        "plotting_pointcloud = np.array(loaded_pointcloud).T\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=plotting_pointcloud[0], y=plotting_pointcloud[1], z=plotting_pointcloud[2],\n",
        "                                   mode='markers', marker=dict(size=1))])\n",
        "\n",
        "\n",
        "fig.update_layout(scene = dict(\n",
        "                    xaxis = dict(title='',\n",
        "                        showgrid=False,\n",
        "                         showticklabels=False),\n",
        "                    yaxis = dict(title='',\n",
        "                         showgrid=False,\n",
        "                         showticklabels=False),\n",
        "                    zaxis = dict(title='',\n",
        "                         showgrid=False,\n",
        "                         showticklabels=False)),\n",
        "                    width=700,\n",
        "                  height= 400\n",
        "                  )\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl5-4ZtPqDgt"
      },
      "source": [
        "loaded_pointcloud = np.load('/content/pointcloud_1024.npy', allow_pickle=True)\n",
        "plotting_pointcloud = np.array(loaded_pointcloud).T\n",
        "fig = go.Figure(data=[go.Scatter3d(x=plotting_pointcloud[0], y=plotting_pointcloud[1], z=plotting_pointcloud[2],\n",
        "                                   mode='markers', marker=dict(size=1))])\n",
        "\n",
        "fig.update_layout(scene = dict(\n",
        "                    xaxis = dict(title='',\n",
        "                        showgrid=False,\n",
        "                         showticklabels=False),\n",
        "                    yaxis = dict(title='',\n",
        "                         showgrid=False,\n",
        "                         showticklabels=False),\n",
        "                    zaxis = dict(title='',\n",
        "                         showgrid=False,\n",
        "                         showticklabels=False)),\n",
        "                    width=700,\n",
        "                  height= 400\n",
        "                  )\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3HDDQoaDlsN"
      },
      "source": [
        "# Normalize pointcloud by subtracting mean and normalizing all points onta a unit sphere.\n",
        "norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "# rotation around z-axis\n",
        "theta = random.random() * 2. * math.pi # rotation angle\n",
        "rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
        "                       [ math.sin(theta),  math.cos(theta),    0],\n",
        "                       [0,                             0,      1]])\n",
        "\n",
        "rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
        "\n",
        "# add some noise\n",
        "noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
        "noisy_pointcloud = rot_pointcloud + noise\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvcS431uEsga"
      },
      "source": [
        "print(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "print(noisy_pointcloud.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv9s-vrh67-n"
      },
      "source": [
        "plotting_pointcloud = np.array(noisy_pointcloud).T\n",
        "fig = go.Figure(data=[go.Scatter3d(x=plotting_pointcloud[0], y=plotting_pointcloud[1], z=plotting_pointcloud[2],\n",
        "                                   mode='markers', marker=dict(size=1))])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "zJEDxIjWri7K",
        "outputId": "2cb95808-041c-41e5-8385-7007cf3a8e55"
      },
      "source": [
        "# Sample code to check some vectorization in tensorflow for a loss function\n",
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/70/45d3b9fab768215a2055c7819d39547a4b0b7401b4583094068741aff99b/tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.36.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.32.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "7V7IDVaLWgLh",
        "outputId": "dac741b1-5bf8-4932-9013-ff757301e579"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.13.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XtdytnZXrJf",
        "outputId": "eb24a3b2-e673-4159-e23d-796bc6d10dd5"
      },
      "source": [
        "%%time\n",
        "## helper functions for k-random-octant loss\n",
        "def compute_l2_norm(pcl_mat, octants):\n",
        "    # pcl_mat: BATCH_SIZE x 1024 x 3 tensor\n",
        "    # octants: BATCH_SIZE k x 3 tensor\n",
        "    # output: BATCH_SIZE x 1024 x k matrix where each row contains l2 norm of each point against each of the k centers.\n",
        "    \n",
        "    ## this is inefficient for larger batches\n",
        "    num_points = pcl_mat.shape[1]\n",
        "    num_octants = octants.shape[1]\n",
        "    distance = np.zeros((num_points, num_octants))\n",
        "    num_batches = pcl_mat.shape[0]\n",
        "    outputTensor = tf.zeros(shape = (1, pcl_mat.shape[1], octants.shape[1]))\n",
        "    for batch in range(num_batches):\n",
        "        output_batch = tf.zeros(shape = (1, pcl_mat.shape[1], 0))\n",
        "        for octant in range(num_octants):\n",
        "            # Tensor(\"ExpandDims_62:0\", shape=(1, 1, 3), dtype=float32)\n",
        "            # Tensor(\"sub_60:0\", shape=(1, 1024, 3), dtype=float32)\n",
        "            # Tensor(\"norm/Sqrt:0\", shape=(1, 1024, 1), dtype=float32)\n",
        "            single_octant_center = tf.expand_dims(\n",
        "                tf.expand_dims(octants[batch, octant, :], \n",
        "                    axis = 0), axis = 0)\n",
        "            diff = pcl_mat[batch, :, :] - single_octant_center\n",
        "            norm_for_all_points = tf.norm(diff, axis = 2, keepdims = True)\n",
        "            output_batch = tf.concat([output_batch, norm_for_all_points], axis = 2)\n",
        "        outputTensor = tf.concat([outputTensor, output_batch], axis = 0)\n",
        "    outputTensor = outputTensor[1:, :, :]\n",
        "    return outputTensor\n",
        "\n",
        "def find_min_distance_to_cluster(distance, pcl):\n",
        "    # distance: 1024 x k matrix where each row contains l2 norm of each point against each of the k centers.\n",
        "    # pcl: 1024 x 3 point clouds\n",
        "    # output: Tuple of a 1xk matrix contains the row index of the min entry and another\n",
        "    # 1 x k matrix where each entry contains the min_distance from a given pointcloud point to the\n",
        "    # kth cluster.\n",
        "    min_distance = tf.reduce_min(distance, axis=1, keepdims=True)\n",
        "    min_distance_row_index = tf.argmin(distance, axis = 1)\n",
        "    \n",
        "    ## gather points for first batch\n",
        "    anchor_points = tf.gather(pcl[0, :, :], min_distance_row_index[0, :])\n",
        "    anchor_points = tf.expand_dims(anchor_points, axis = 0)\n",
        "\n",
        "    ## stack the rest\n",
        "    for i in range(1, pcl.shape[0]):\n",
        "        anchor_points = tf.concat([anchor_points, tf.expand_dims(tf.gather(pcl[i, :, :], min_distance_row_index[i, :]), axis = 0)], axis = 0)\n",
        "    return anchor_points, min_distance_row_index\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 8.82 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLQ38XAIV7H4",
        "outputId": "5f43b6a8-6ed3-4559-ef5d-62ce39afd4d2"
      },
      "source": [
        "%%time\n",
        "def getAnchorPoints(pcl_tensor, k):\n",
        "    '''\n",
        "    pcl_tensor: tensor of dim = (BS,N_pts,3)\n",
        "    '''\n",
        "    min_vals = tf.reduce_min(pcl_tensor, axis = 1)  \n",
        "    max_vals = tf.reduce_max(pcl_tensor, axis = 1)\n",
        "    intervals = (max_vals - min_vals)/k\n",
        "    xyz_inits = min_vals - intervals/2\n",
        "\n",
        "    # tf_range = tf.range(xyz_inits, max_vals, delta=intervals)\n",
        "\n",
        "    # with tf.Session() as s:\n",
        "    #   print('MIN: ', s.run(min_vals))\n",
        "    #   print('MAX: ', s.run(max_vals))\n",
        "    #   print('Intervals: ', s.run(intervals))\n",
        "    #   print('INITS: ', s.run(xyz_inits))\n",
        "    #   print('Range: ', s.run(tf_range))\n",
        "    # init_vals = \n",
        "    \n",
        "    ## this works but it's inefficient for large values of k\n",
        "    batchArray = []\n",
        "    for batch in range(pcl_tensor.shape[0]):\n",
        "        \n",
        "        x_min, y_min, z_min = min_vals[batch, 0], min_vals[batch, 1], min_vals[batch, 2]\n",
        "        # print(x_min, y_min, z_min)\n",
        "        x_max, y_max, z_max = max_vals[batch, 0], max_vals[batch, 1], max_vals[batch, 2]\n",
        "        x_interval, y_interval, z_interval = intervals[batch, 0], intervals[batch, 1], intervals[batch, 2]\n",
        "        ## compute geometric centers\n",
        "        searchVolumes = []\n",
        "        overallIndex = 1\n",
        "        x_init, y_init, z_init = x_min - x_interval/2, y_min - y_interval/2, z_min - z_interval/2\n",
        "        for p in range(0, k):\n",
        "            x_init += x_interval\n",
        "            for q in range(0, k):\n",
        "                y_init += y_interval\n",
        "                for r in range(0, k):\n",
        "                    z_init += z_interval\n",
        "                    searchVolumes.append([x_init, y_init, z_init])\n",
        "                    overallIndex += 1\n",
        "                z_init = z_min - z_interval/2\n",
        "            y_init = y_min - y_interval/2\n",
        "                \n",
        "        batchArray.append(searchVolumes)\n",
        "    octants = tf.convert_to_tensor(batchArray)\n",
        "\n",
        "    ## get anchor points' distances\n",
        "    ## L2_norm([1024, 3], [27, 3]) ==> [1024, 27] all point clouds dist to all geometric centers ==> [1, 27]\n",
        "    distance = compute_l2_norm(pcl_tensor, octants)\n",
        "    return find_min_distance_to_cluster(distance, pcl_tensor)\n",
        "\n",
        "def getCenterPoint(pcl_tensor):\n",
        "    min_vals = tf.reduce_min(pcl_tensor, axis = 1)\n",
        "    max_vals = tf.reduce_max(pcl_tensor, axis = 1)\n",
        "    geometric_center = tf.expand_dims(min_vals + (max_vals - min_vals)/2, axis = 1)\n",
        "    l2_norms = compute_l2_norm(pcl_tensor, geometric_center)\n",
        "    return find_min_distance_to_cluster(l2_norms, pcl_tensor)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 0 ns, sys: 10 µs, total: 10 µs\n",
            "Wall time: 14.5 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h59iHQdAWivs",
        "outputId": "15b05584-3873-49e2-ac37-7145eca08102"
      },
      "source": [
        "%%time\n",
        "# Create a random tensor of size (BS, N, 3)\n",
        "gt = tf.random.uniform(shape=(2, 1024, 3))\n",
        "\n",
        "# with tf.Session() as s:\n",
        "  # print('GT: ', s.run(gt))\n",
        "getAnchorPoints(gt, k=3)\n",
        "getAnchorPoints(gt, k=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.61 s, sys: 15.8 ms, total: 1.63 s\n",
            "Wall time: 1.63 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG9py2lXXZ-D",
        "outputId": "ffd4b54e-9d78-4fcf-e492-a76551d2fa22"
      },
      "source": [
        "print(gt_anchor_points)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"concat_284:0\", shape=(2, 27, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ce-c1IOYnpp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}